{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "import sys\n",
    "import requests\n",
    "import re\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_session_setup():\n",
    "    \"\"\"\n",
    "    creates a spark context\n",
    "    >>> sc = spark_session_setup()\n",
    "    \"\"\"\n",
    "    # in order to be bale to change log level\n",
    "    conf = ps.SparkConf()\n",
    "    conf.set('spark.logConf', 'true')\n",
    "    conf.set('spark.executor.memory', '12G')\n",
    "    conf.set('spark.driver.memory', '12G')\n",
    "#     conf.set('spark.driver.maxResultSize', '10G')\n",
    "    # create a spark session\n",
    "    sc = ps.SparkContext(appName='word_count', conf=conf)\n",
    "    # change log level to ERROR\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    return sc\n",
    "sc = spark_session_setup()\n",
    "\n",
    "sc = SparkContext.getOrCreate()#SparkConf().setMaster(\"local[*]\"))\n",
    "sql_context = ps.sql.SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asm_data_path = 'https://storage.googleapis.com/uga-dsp/project1/data/asm/'\n",
    "byte_data_path = 'https://storage.googleapis.com/uga-dsp/project1/data/bytes/'\n",
    "x_small_train_path ='https://storage.googleapis.com/uga-dsp/project1/files/X_small_train.txt'\n",
    "y_small_train_path ='https://storage.googleapis.com/uga-dsp/project1/files/y_small_train.txt'\n",
    "x_small_test_path ='https://storage.googleapis.com/uga-dsp/project1/files/X_small_test.txt'\n",
    "y_small_test_path ='https://storage.googleapis.com/uga-dsp/project1/files/y_small_test.txt'\n",
    "text = requests.get(x_small_train_path).text\n",
    "data = sc.parallelize(text.splitlines(),numSlices=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = requests.get(x_small_train_path).text.split('\\n')\n",
    "labels = requests.get(y_small_train_path).text.split('\\n')\n",
    "filename_label_dict = {}\n",
    "for filename, label in zip(filenames, labels):\n",
    "    filename_label_dict[filename] = label\n",
    "\n",
    "broadcast_filename_label_dict = sc.broadcast(filename_label_dict)\n",
    "\n",
    "def add_asm_texts_to_features(x): \n",
    "    path = asm_data_path+x+'.asm'\n",
    "    text1 = requests.get(path).text.splitlines()\n",
    "    text2 = [element.partition(':')[0] for element in text1]\n",
    "    fname = x\n",
    "    label = int(broadcast_filename_label_dict.value[fname])\n",
    "    return((fname,label,text2))\n",
    "\n",
    "\n",
    "train_data_with_asm=data.map(lambda x: add_asm_texts_to_features(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = requests.get(x_small_test_path).text\n",
    "test_data = sc.parallelize(text_test.splitlines(),numSlices=80)\n",
    "\n",
    "filenames_test = requests.get(x_small_test_path).text.split('\\n')\n",
    "labels_test = requests.get(y_small_test_path).text.split('\\n')\n",
    "filename_label_dict_test = {}\n",
    "for filename, label in zip(filenames_test, labels_test):\n",
    "    filename_label_dict_test[filename] = label\n",
    "\n",
    "broadcast_filename_label_dict_test = sc.broadcast(filename_label_dict_test)\n",
    "\n",
    "def add_asm_texts_to_features_test(x): \n",
    "    path = asm_data_path+x+'.asm'\n",
    "    text1 = requests.get(path).text.splitlines()\n",
    "    text2 = [element.partition(':')[0] for element in text1]\n",
    "    fname = x\n",
    "    label = int(broadcast_filename_label_dict_test.value[fname])\n",
    "    return((fname,label,text2))\n",
    "\n",
    "\n",
    "test_data_with_asm=test_data.map(lambda x: add_asm_texts_to_features_test(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df = sql_context.createDataFrame(test_data_with_asm, ['doc', 'label', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = sql_context.createDataFrame(train_data_with_asm, ['doc', 'label', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training: Tokenize, Frequency, TF-IDF\n",
    "# remover = StopWordsRemover(inputCol=\"text\", outputCol='filtered', stopWords=['??'])#, '00'])\n",
    "ngram = NGram(n=3, inputCol='text', outputCol='ngrams')\n",
    "hashingTF = HashingTF(inputCol=\"ngrams\", outputCol=\"features\") #, numFeatures=256)\n",
    "#idf = IDF(inputCol='freqs', outputCol='features')\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "#ML Pipeline Model\n",
    "pipeline = Pipeline(stages=[ngram, hashingTF, nb])\n",
    "model = pipeline.fit(train_data_df)\n",
    "#model.save('NB_Best_Model')\n",
    "predictions = model.transform(test_data_df)\n",
    "\n",
    "#Evaluate Model Accuracy\n",
    "\n",
    "predictions = predictions.withColumn('label',predictions['label'].cast(DoubleType()))\n",
    "add_one= functions.udf(lambda x:x+1)\n",
    "predictions=predictions.withColumn('addedprediction',add_one('prediction'))\n",
    "predictions = predictions.withColumn('addedprediction',predictions['addedprediction'].cast(DoubleType()))\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"addedprediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code gives 33.13% accuracy , which is pretty bad.. lets tweak and few things and see what happens..//ran on gcp ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
