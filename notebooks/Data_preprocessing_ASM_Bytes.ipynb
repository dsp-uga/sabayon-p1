{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "import sys\n",
    "import requests\n",
    "import re\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session Setup : \n",
    "In order to deal with memory issues, we created a seperate method to initialize executor memory and driver memory.This solution is particularly good for jupyter notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_session_setup():\n",
    "#     \"\"\"\n",
    "#     creates a spark context\n",
    "#     >>> sc = spark_session_setup()\n",
    "#     \"\"\"\n",
    "    # in order to be bale to change log level\n",
    "    conf = ps.SparkConf()\n",
    "    conf.set('spark.logConf', 'true')\n",
    "    conf.set('spark.executor.memory', '12G')\n",
    "    conf.set('spark.driver.memory', '12G')\n",
    "    # create a spark session\n",
    "    sc = ps.SparkContext(appName='word_count', conf=conf)\n",
    "    # change log level to ERROR\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    return sc\n",
    "\n",
    "sc = spark_session_setup()\n",
    "sql_context = ps.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_broadcast_label_dict() : \n",
    "#     '''This method is used to get a dictionary of filenames and their corresponding labels for what malware they represent. \n",
    "#     This dictionary enables an efficient use of map function later on where we are able to create a new column of labels in an RDD \n",
    "#     quite easily. \n",
    "#     '''\n",
    "    filenames = requests.get(x_small_train_path).text.split('\\n')\n",
    "    labels = requests.get(y_small_train_path).text.split('\\n')\n",
    "    filename_label_dict = {}\n",
    "    for filename, label in zip(filenames, labels):\n",
    "        filename_label_dict[filename] = label\n",
    "    return sc.broadcast(filename_label_dict)\n",
    "\n",
    "def find_file(x): \n",
    "   # This is used to map a file name to its corresponding text in byte file \n",
    "    path = byte_data_path+x+'.bytes'\n",
    "    text1 = requests.get(path).text\n",
    "    return(x,text1)\n",
    "\n",
    "\n",
    "def pre_process(x):\n",
    "    # This method is used to preprocess the data and also add labels as a separate column. \n",
    "    fname = x[0]\n",
    "    label = int(broadcast_filename_label_dict.value[fname])\n",
    "    word_list = list(filter(lambda x: len(x)==2 and x!='??', re.split('\\r\\n| ', x[1])))\n",
    "    return (fname, label, word_list)\n",
    "\n",
    "def pre_process_test(x):\n",
    "    fname = x[0]\n",
    "    label = int(broadcast_filename_label_dict_test.value[fname])\n",
    "    word_list = list(filter(lambda x: len(x)==2 and x!='??', re.split('\\r\\n| ', x[1])))\n",
    "    return (fname, label, word_list)\n",
    "\n",
    "def add_asm_texts_to_features(x): \n",
    "\n",
    "    #this method is used to add asm features into rdd\n",
    "\n",
    "    path = asm_data_path+x[0]+'.asm'\n",
    "    text1 = requests.get(path).text.splitlines()\n",
    "    text2 = [element.partition(':')[0] for element in text1]\n",
    "    text2.extend(x[2])\n",
    "    return((x[0],x[1],text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a setup for GCP cluster, where we can initialize executor and driver memories while pyspark command. \n",
    "sc = SparkContext.getOrCreate()\n",
    "#SparkConf().setMaster(\"local[*]\"))\n",
    "sql_context = ps.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_paths for asm and byte raw data, which have name of file and their respective asma nd byte information. \n",
    "asm_data_path = 'https://storage.googleapis.com/uga-dsp/project1/data/asm/'\n",
    "byte_data_path = 'https://storage.googleapis.com/uga-dsp/project1/data/bytes/'\n",
    "\n",
    "#In order to access that raw data, we need references to use them as what? train or test??\n",
    "#This was provided by our project requirements and links for respective filenames for X and Y are given below : \n",
    "\n",
    "x_small_train_path ='https://storage.googleapis.com/uga-dsp/project1/files/X_small_train.txt'\n",
    "y_small_train_path ='https://storage.googleapis.com/uga-dsp/project1/files/y_small_train.txt'\n",
    "x_small_test_path ='https://storage.googleapis.com/uga-dsp/project1/files/X_small_test.txt'\n",
    "y_small_test_path ='https://storage.googleapis.com/uga-dsp/project1/files/y_small_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing first rdd with files names given in X_small_train path.\n",
    "\n",
    "text = requests.get(x_small_train_path).text\n",
    "# number of partitions are given using numSlices, which is taken to be 80. This increased speed as all stages were futher divided \n",
    "#into multiple tasks where each partition corresponded with a partition.  \n",
    "data = sc.parallelize(text.splitlines(),numSlices=80)\n",
    "# used to take 1 row instance of data to show the contents of rdd.\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "broadcast_filename_label_dict = get_broadcast_label_dict()\n",
    "train_data=data.map(lambda x: find_file(x))\n",
    "train_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_labels=train_data.map(lambda x: pre_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_labels.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identical processes are used to create a test rdd as well. \n",
    "text_test = requests.get(x_small_test_path).text\n",
    "test_data = sc.parallelize(text_test.splitlines(),numSlices=80)\n",
    "\n",
    "filenames_test = requests.get(x_small_test_path).text.split('\\n')\n",
    "labels_test = requests.get(y_small_test_path).text.split('\\n')\n",
    "filename_label_dict_test = {}\n",
    "for filename, label in zip(filenames_test, labels_test):\n",
    "    filename_label_dict_test[filename] = label\n",
    "\n",
    "broadcast_filename_label_dict_test = sc.broadcast(filename_label_dict_test)\n",
    "\n",
    "test_data_new=test_data.map(lambda x: find_file(x))\n",
    "\n",
    "test_data_with_labels=test_data_new.map(lambda x: pre_process_test(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_labels.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_asm features is mapped to rdd\n",
    "test_data_with_asm=test_data_with_labels.map(lambda x: add_asm_texts_to_features(x))\n",
    "test_data_with_asm.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test dataset formed into a dataframe\n",
    "test_data_df_repar = sql_context.createDataFrame(test_data_with_asm, ['doc', 'label', 'text'])\n",
    "test_data_df_repar.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a check for number of partitions, which is not expected to change. \n",
    "test_data_df_repar.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_labels.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asm added into train_data rdd/\n",
    "train_data_with_asm=train_data_with_labels.map(lambda x: add_asm_texts_to_features(x))\n",
    "train_data_with_asm.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data rdd formed into dataframe\n",
    "train_data_df = sql_context.createDataFrame(train_data_with_asm, ['doc', 'label', 'text'])\n",
    "train_data_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiemntation done of various components of mllib pipeline. \n",
    "\n",
    "# ngram = NGram(n=1, inputCol='text', outputCol='ngrams')\n",
    "# ngramed_df= ngram.transform(train_data_df)\n",
    "# hashingTF = HashingTF(inputCol=\"ngrams\", outputCol=\"features\")\n",
    "# hashedTF=hashingTF.transform(ngramed_df)\n",
    "# hashedTF.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training: Tokenize, Frequency, TF-IDF\n",
    "\n",
    "# remover = StopWordsRemover(inputCol=\"text\", outputCol='filtered', stopWords=['??'])#, '00'])\n",
    "ngram = NGram(n=1, inputCol='text', outputCol='ngrams')\n",
    "hashingTF = HashingTF(inputCol=\"ngrams\", outputCol=\"features\") #, numFeatures=256)\n",
    "#idf = IDF(inputCol='freqs', outputCol='features')\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "#ML Pipeline Model\n",
    "pipeline = Pipeline(stages=[ngram, hashingTF, nb])\n",
    "model = pipeline.fit(train_data_df)\n",
    "#model.save('NB_Best_Model')\n",
    "predictions = model.transform(test_data_df_repar)\n",
    "\n",
    "#Evaluate Model Accuracy\n",
    "\n",
    "predictions = predictions.withColumn('label',predictions['label'].cast(DoubleType()))\n",
    "add_one= functions.udf(lambda x:x+1)\n",
    "predictions=predictions.withColumn('addedprediction',add_one('prediction'))\n",
    "predictions = predictions.withColumn('addedprediction',predictions['addedprediction'].cast(DoubleType()))\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"addedprediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GETTING 64.497% accuracy after adding one to prediction labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
