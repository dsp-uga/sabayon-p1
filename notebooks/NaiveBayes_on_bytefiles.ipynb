{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes is implemented on Byte Data \n",
    "This document just uses bytes data, which are hexadecimal numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "import sys\n",
    "import requests\n",
    "import re\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methods\n",
    "\n",
    "def spark_session_setup():\n",
    "    #     \"\"\"\n",
    "#     creates a spark context\n",
    "#     >>> sc = spark_session_setup()\n",
    "#     \"\"\"\n",
    "    # in order to be bale to change log level\n",
    "    conf = ps.SparkConf()\n",
    "    conf.set('spark.logConf', 'true')\n",
    "    conf.set('spark.executor.memory', '12G')\n",
    "    conf.set('spark.driver.memory', '12G')\n",
    "#     conf.set('spark.driver.maxResultSize', '10G')\n",
    "    # create a spark session\n",
    "    sc = ps.SparkContext(appName='word_count', conf=conf)\n",
    "    # change log level to ERROR\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    return sc\n",
    "\n",
    "\n",
    "\n",
    "def get_broadcast_label_dict() : \n",
    "#     '''This method is used to get a dictionary of filenames and their corresponding labels for what malware they represent. \n",
    "#     This dictionary enables an efficient use of map function later on where we are able to create a new column of labels in an RDD \n",
    "#     quite easily. \n",
    "#     '''\n",
    "    filenames = requests.get(x_small_train_path).text.split('\\n')\n",
    "    labels = requests.get(y_small_train_path).text.split('\\n')\n",
    "    filename_label_dict = {}\n",
    "    for filename, label in zip(filenames, labels):\n",
    "        filename_label_dict[filename] = label\n",
    "    return sc.broadcast(filename_label_dict)\n",
    "\n",
    "def find_file(x): \n",
    " # This is used to map a file name to its corresponding text in byte file \n",
    "    path = byte_data_path+x+'.bytes'\n",
    "    text1 = requests.get(path).text\n",
    "    return(x,text1)\n",
    "\n",
    "\n",
    "def pre_process(x):\n",
    "     # This method is used to preprocess the data and also add labels as a separate column. \n",
    "    fname = x[0]\n",
    "    label = int(broadcast_filename_label_dict.value[fname])\n",
    "    word_list = list(filter(lambda x: len(x)==2 and x!='??', re.split('\\r\\n| ', x[1])))\n",
    "    return (fname, label, word_list)\n",
    "\n",
    "\n",
    "def get_broadcast_label_dict_test() : \n",
    "#     '''This method is used to get a dictionary of filenames and their corresponding labels for what malware they represent. \n",
    "#     This dictionary enables an efficient use of map function later on where we are able to create a new column of labels in an RDD \n",
    "#     quite easily. \n",
    "#     '''\n",
    "    filenames_test = requests.get(x_small_test_path).text.split('\\n')\n",
    "    labels_test = requests.get(y_small_test_path).text.split('\\n')\n",
    "    filename_label_dict_test = {}\n",
    "    for filename, label in zip(filenames_test, labels_test):\n",
    "        filename_label_dict_test[filename] = label\n",
    "    \n",
    "    return sc.broadcast(filename_label_dict_test)\n",
    "\n",
    "\n",
    "def pre_process_test(x):\n",
    "    fname = x[0]\n",
    "    label = int(broadcast_filename_label_dict_test.value[fname])\n",
    "    word_list = list(filter(lambda x: len(x)==2 and x!='??', re.split('\\r\\n| ', x[1])))\n",
    "    return (fname, label, word_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session Setup : \n",
    "In order to deal with memory issues, we created a seperate method to initialize executor memory and driver memory.This solution is particularly good for jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc = spark_session_setup()\n",
    "\n",
    "sc = SparkContext.getOrCreate()#SparkConf().setMaster(\"local[*]\"))\n",
    "sql_context = ps.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_paths for asm and byte raw data, which have name of file and their respective asma nd byte information. \n",
    "asm_data_path = 'https://storage.googleapis.com/uga-dsp/project1/data/asm/'\n",
    "byte_data_path = 'https://storage.googleapis.com/uga-dsp/project1/data/bytes/'\n",
    "\n",
    "#In order to access that raw data, we need references to use them as what? train or test??\n",
    "#This was provided by our project requirements and links for respective filenames for X and Y are given below : \n",
    "\n",
    "x_small_train_path ='https://storage.googleapis.com/uga-dsp/project1/files/X_small_train.txt'\n",
    "y_small_train_path ='https://storage.googleapis.com/uga-dsp/project1/files/y_small_train.txt'\n",
    "x_small_test_path ='https://storage.googleapis.com/uga-dsp/project1/files/X_small_test.txt'\n",
    "y_small_test_path ='https://storage.googleapis.com/uga-dsp/project1/files/y_small_test.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing first rdd with files names given in X_small_train path.\n",
    "\n",
    "text = requests.get(x_small_train_path).text\n",
    "\n",
    "# number of partitions are given using numSlices, which is taken to be 80. This increased speed as all stages were futher divided \n",
    "#into multiple tasks where each partition corresponded with a partition.  \n",
    "data = sc.parallelize(text.splitlines(),numSlices=80)\n",
    "# used to take 1 row instance of data to show the contents of rdd.\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#broadcasting the dictionary of labels with filenames\n",
    "broadcast_filename_label_dict = get_broadcast_label_dict()\n",
    "\n",
    "#adding a column with file text to train_data rdd. \n",
    "train_data=data.map(lambda x: find_file(x))\n",
    "train_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying preprocessing on train data and also adding labels. \n",
    "train_data_with_labels=train_data.map(lambda x: pre_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting names test data elements.. \n",
    "text_test = requests.get(x_small_test_path).text\n",
    "test_data = sc.parallelize(text_test.splitlines(),numSlices=80)\n",
    "# broadcasting dictionary of test labels.    \n",
    "broadcast_filename_label_dict_test = get_broadcast_label_dict_test()\n",
    "# finding the text file and adding its column of test data rdd\n",
    "test_data_new=test_data.map(lambda x: find_file(x))\n",
    "# preprocessing the test data text and also adding labels. \n",
    "test_data_with_labels=test_data_new.map(lambda x: pre_process_test(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe from test rdd..\n",
    "test_data_df = sql_context.createDataFrame(test_data_with_labels, ['doc', 'label', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe from train rdd..\n",
    "train_data_df = sql_context.createDataFrame(train_data_with_labels, ['doc', 'label', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training: Tokenize, Frequency, TF-IDF\n",
    "# remover = StopWordsRemover(inputCol=\"text\", outputCol='filtered', stopWords=['??'])#, '00'])\n",
    "ngram = NGram(n=3, inputCol='text', outputCol='ngrams')\n",
    "hashingTF = HashingTF(inputCol=\"ngrams\", outputCol=\"features\") #, numFeatures=256)\n",
    "#idf = IDF(inputCol='freqs', outputCol='features')\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "#ML Pipeline Model\n",
    "pipeline = Pipeline(stages=[ngram, hashingTF, nb])\n",
    "model = pipeline.fit(train_data_df)\n",
    "#model.save('NB_Best_Model')\n",
    "predictions = model.transform(test_data_df)\n",
    "\n",
    "#Evaluate Model Accuracy\n",
    "\n",
    "predictions = predictions.withColumn('label',predictions['label'].cast(DoubleType()))\n",
    "add_one= functions.udf(lambda x:x+1)\n",
    "predictions=predictions.withColumn('addedprediction',add_one('prediction'))\n",
    "predictions = predictions.withColumn('addedprediction',predictions['addedprediction'].cast(DoubleType()))\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"addedprediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
